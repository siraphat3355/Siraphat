% set 0 inch indentation
\setlength{\parindent}{0in} 
% set paragraph space = 1 space
\setlength{\parskip}{1em}
% set line space 1.5
\setlength{\baselineskip}{1.6em}


\chapter{Literature Review} 
\label{ch:literature-review}

\paragraph{}
Nowadays, Falls are concernable problem around the world. Fall detection is an interested topic that researchers prefer to receive the best accuracy. Several methods have tried to overcome this problem, but they have suffered with a lot of constrains. Nonetheless, using vibration signal to detect fall actions may highly modernize in order to mitigate senile fall problem.

\paragraph{}
There are several knowledge related fields which start from vibration untill artificial intelligence model, and every section of this system as software and hardware are equally important. Thus, we have to explore and deeply understand in each branch in order to build the best system.


\section{Fall}
\label{Fall}
\paragraph{}
Falls happen to people of all ages, but older people have a high probability of being  harmed and are more likely to fall, especially if they have an abnormal health conditions or balance problems. Falls are a common but often disregarded cause of injury. According to \citeauthor{nhs_2019} \citeyear{nhs_2019} one in three adults over 65 and half of the people over 80 have at least one fall per year. Most falls do not result in serious injury, but there is always a risk that a fall could lead to broken bones, and it can cause the person to have paralysis. In addition, the level of injury depends on the timeliness of the assistance. Unintentional falls can cause severe injuries and even death, especially if no immediate assistance is given.

\subsection{Fall Detection}
\paragraph{}
Global trends in fall detection are illustrated in Figure \ref{fig:fall_trend}. The data are downloaded from Google Trends with the search topic ``Fall Detection". Fall detection has gotten increasingly more attention over time and significantly increased in 2019. The values are indexed to be 100, where 100 is the maximum search interest for that period of time with specific location. Researchers have developed systems using a variety of different sensors and methods depending on their proposes and technological industry. Consequently, we can conclude that this topic is of interest and is becoming increasingly popular.


\begin{figure}[H]
  \centering
  \caption[Fall detection trends]{\emph{Interest in “Fall Detection” over time from 2004 to present according to Google Trends.}}\label{fig:fall_trend}
  \includegraphics[width=\textwidth]{figures/fall_trend.png}  
\end{figure}

\subsection{Fall detection by using vibration sensors}
\paragraph{}
Table \ref{tab:fall_review} shows the evolution of fall detection from floor vibration. Most researches use classifier models to detect fall events with training performed on simulated fall data that were not real falls. Furthermore, none of these researchers have deployed their system in real environments, so the real world performance of the models is not convincing. To overcome these weaknesses, I will apply anomaly detection to compensate for the rarity of events such as real falls and other or unseen patterns, and send alerts to the caretaker or an assistance who can take care of a victim who is alone as soon as possible.

\begin{table}[H]
\begin{center}
% \linespread{0.55}\selectfont\centering
\caption[Summary of literature review for fall detection from floor vibration.]{Summary of literature review for fall detection from floor vibration. \\}\label{tab:fall_review}
\begin{tabular}{m{0.13\linewidth} m{0.23\linewidth} m{0.18\linewidth} m{0.18\linewidth} m{0.2\linewidth} }
  \textbf{Authors} & \textbf{Data Collection} & \textbf{Sensors} & \textbf{Algorithms} & \textbf{Alarm} \\
\hline

\shortciteA{Alwan_2003} & Simulated by people. &  N/A & Threshold & N/A \\
\hline
   \shortciteA{alwan_rajendran_kell_mack_dalal_wolfe_felder_2006} & Simulated by people and dummies. & Piezoelectric & Threshold  & Send messages to a pager \\
\hline

\shortciteA{litvak_zigel_gannot_2008} & Simulated by people and dummies. & Microphone \newline Accelerometer & Gaussian model \newline Sequential forward floating selection (SFFS) \raggedright & N/A \\
\hline


\shortciteA{davis_caicedo_langevin_hirth_2011} & Simulated by people. & N/A & Threshold & N/A \\
\hline

\shortciteA{inproceedings} & N/A & Pyroelectric infeared (PIR) \newline Vibration sensor & Support vector machine (SVM) \raggedright & N/A \\
\hline


\shortciteA{shao_wang_song_ilyas_guo_chang_2020} & Simulated by 3d-printed skeleton \raggedright & Accelerometer on smartphone & K-nearest-neighbor (KNN) & N/A \\
\hline

\shortciteA{liu_jiang_su_benzoni_maxwell_2019} & Simulated by people and dummies. & Seismic  &  A multi-features semi-supervised support vector machines (MFSS - SVM) \raggedright & N/A \\
\hline

\shortciteA{clemente_li_valero_song_2020} & N/A & Seismic & One-class SVM & N/A \\
\hline

\shortciteA{mukherjee2020multisense} & N/A &  Motion sensor \newline Heat sensor \newline Vibration sensor & Threshold & N/A \\
\hline

   \end{tabular}
\end{center}
 \end{table}

\section{Human Activity}
\paragraph{}
To create training data for anomaly detection in the home, it is important to cover all of the typical activities that people are expected to engage in in the home.

\paragraph{}
\citeauthor{schrader_2020} \citeyear{schrader_2020}  say there is no common definition or description of human activities because human activity is highly diverse. Nonetheless, the most fundamental activity in home is clearly walking, since a resident needs to move several inside the home to perform any other activities \cite{oukrich_2019}. There also are other general activities that every person does. I summarized activity catagories proposed in the literature on human activity in home in Table \ref{tab:human_activity}. Each paper in the table includes experiments on different activities of interest, and there are some common activities across most of the studies such as sitting, walking, standing and lying.

\begin{table}[H]
\begin{center}
\caption[Summary of literature review on human activity.]{\emph{Summary of literature review on human activity.}} \label{tab:human_activity}
\begin{tabular}{m{0.2\textwidth} m{0.3\textwidth} m{0.2\textwidth} m{0.25\textwidth} }
  \textbf{Authors} & \textbf{Objective of study} & \textbf{Related Sensors} & \textbf{Identified Activities} \\
\hline

\shortciteA{roggen_2010} \raggedright & Collect complex activity datasets in home \raggedright & Microphone \newline Accelerometers \newline Gyroscope \newline Magnetometer \newline Inertial sensor & Sitting  \newline Walking \newline Standing \newline Lying \\
\hline

\cite{chen_xue_2015} \raggedright & Classify human activity by single accelerometer \raggedright  & Accelerometer &  Walking \newline Standing \newline Lying \newline Running \newline Rope jump \newline Vacuum cleaning \newline Downstairs \newline Upstairs \\
\hline

\cite{reiss_stricker_2012} \raggedright & Published a new public dataset for physical activity \raggedright & Gyroscope \newline Magnetometer &  Sitting \newline Step walking \newline Walking quickly \newline Falling \newline Jumping \newline Running \newline Downstairs \newline Upstairs \\
\hline

\shortciteA{ugolotti_sassi_mordonini_cagnoni_2011} \raggedright & Detect and classify human activities \raggedright & Camera \newline Accelerometer & Sitting \newline Walking \newline Standing \newline Lying \newline Get up \newline Fall \newline Rise \\
\hline

\cite{abbate_avvenuti_bonatesta_cola_corsini_vecchio_2012} \raggedright & Detect fall events & Accelerometer on smartphone & Sitting \newline Walking \newline Lying \newline Running \newline Jumping \newline Hitting the sensor \\
\hline

   \end{tabular}
\end{center}
\end{table}


\section{Floor Vibrations}
\paragraph{}
Movement in a building by residents during their normal activities causes floor vibration. This vibration is normally vertical \cite{steelconstruction_2016}. Floor vibrations are generated by dynamic loads caused directly by people (e.g. walking, dancing, jumping) or machinery, or they may be generated indirectly by the external environment (e.g. traffic). Theoretically, vibrations are cyclic motions with two significant attributes, frequency and amplitude. In practice, floor vibrations are quite complex dynamic systems with unlimited vibrational modes. \citeauthor{ljunggren2006floor} \citeyear{ljunggren2006floor} summarises the parameter that influence the dynamic system of a floor:
\begin{itemize}
\item Stiffness ($k$): Stiffness controls the springiness of the floor. Higher stiffness can decreases the vibrational amplitude occurring due to a force.
\item Damping ($\zeta$): This factor depends on the material making up the surface. It is extremely difficult to obtain an exact damping value.
\item Mass ($M$): Higher mass surfaces have reduced vibrational amplitudes. Lower mass is desirable if we want to observe vibrations. However, when the mass is too little, the resulting strong vibrations may disturb residents.
\item Fundamental frequency: Floor vibrations are assumed to be occur mainly at a natural frequency, which depends on the stiffness and the mass. Higher frequencies are usually less annoying to residents than lower frequencies.\paragraph{}
\end{itemize}
\paragraph{}
The complexities of the dynamic system can be modeled as a series of simple mass and spring models with a single degree of freedom \cite{p_gavin_2015}. The characteristics of a vibration model are illustrated in Figure \ref{fig:s_degree}.

\begin{figure}[H]
  \centering
  \caption[Single degree of freedom system mass - spring model for floor vibration.]{\emph{Single degree of freedom system mass - spring model for floor vibration. \\ Reprinted from \citeauthor{steelconstruction_2016} \citeyear{steelconstruction_2016}.}  }\label{fig:s_degree}
  \includegraphics[scale = 0.13]{figures/single_degree.jpg}  
\end{figure}

\section{Time Series}
\paragraph{}
A time series is a sequence of measurements of a particular random variable at specific sequence of discrete points in time. Generally, the data should be sampled at a constant interval expresssed in as seconds, minutes, hours, days, months, and/or years. In time series analysis, we would generally like to predict a traget variable at particular time lags given a window of previous measurements. This is unusual in that in ordinary supervised classification or regression, the target at time $t$ is not used as a feature at a later time, but in time series analysis, this is often the case. 

\paragraph{}
There are many diverse techniques for analyzing sequential data. The simplest techniques are a special case of regression analysis in which we want to capture four different elements as following \cite{dash_2020}:
\begin{itemize}
\item Seasonal variations: Repeating shape or appearance occuring during a specific period such as daily, weekly, monthly, or seasonally.
\item Trend: Possible trends are upwards, downwards, or constantard can be linear or nonlinear.
\item Cyclical variations: Movement that follows a specific cyclic period such as business cycles. Cyclical variations are similar to seasonal variations but have different underlying cases specific to the particular problem.
\item Random variations: The variation remaining after the first three types of predictable variation are accounted for.
\end{itemize}

\subsection{Autoregressive (AR)}
\paragraph{}
Autoregressive models, the simplest time series models, which are used for predict or forecasting proposes, operate under the assumption that each new value depends on some or all of the the past values. The generative model for a linear of autoregressive process is shown below:

\hfil $Y_t = \varphi_1Y_{t-1} + \cdots + \varphi_pY_{t-p} + \epsilon_t $ \par 

where $\epsilon \sim N(0, \sigma^2)$. p is the order of the model, which we write as AR(p). For example, AR(1) means the observation at time $t$ depends only on the observation at time $t-1$ plus noise, whereare. AR(2) means $y_t$ depends on the previous two values as well as a noise sample.

\subsection{Time series classification}
\paragraph{}
Over the last two decades, one of the most challenging problems in data mining is a classification of time series \cite{ismail_fawaz_forestier_weber_idoumghar_muller_2019}. Several methods have emerged for time series classification. The naive algorithm is Euclidean maching, which is not normally effective without some modification. On the other hand, dynamic time warping (DTW) is an outstanding baseline, and the current state of the art would in the most cases be represented by deep learning classifiers. Dynamic time warping is based on an alignment cost computed between two data sequences that can be stretched or shrunk to accommodate variations along the time axis \cite{meinard_2007, toyoda_sakurai_2012}. Consider two sequences, $X = (x_1, x_2, ..., x_n)$ of length $n$ and $Y = (y_1, y_2, ..., y_m)$ of length $m$. The DTW distance $D(X,Y)$ is defined as: 

\hfil $ D(X, Y) = D(m, n)$ \par 
\hfil $ D(i,j) = (x_i - y_j)^2$ + $\min$ $\begin{cases} $D(i-1, j)$ \\ $D(i-1, j-1)$ \\$D(i, j-1)$ \end{cases}$  \par 
where $D(0, 0) = 0$, $D(i, 0) = D(j, 0) = \infty$, $i = (1, 2, ..., n)$ and $j = (1,2, ...,m)$.


\begin{figure}[H]
  \centering
  \caption[Euclidean maching versus DTW matching.]{\emph{Euclidean maching versus DTW matching. \\
  Reprinted from Dynamic time warping \cite{dtw_2021}.}}\label{fig:DTW}
  \includegraphics[scale = 0.5]{figures/DTW.jpg}  
\end{figure}


\paragraph{}
DTW can be used not only for pattern matching or classification, but also for anomaly detection. If the distance between a new signal and each signal in a gallery of historical signals is higher than a set threshold, we can conclude the new signal is an anomaly. The main weaknesses of dynamic time warping is its long processing time. Some more effective learning-based approaches are explained in the following sections.

\subsection{Convolutional Neural Network (CNN)}
\paragraph{}
A Convolutional neural network is a deep learning model whose input can be an image, video, spatial data, or any multidimensional tensor with locality. One-dimensional CNNs can be used on general data types including text tokens and other types of time series data. CNNs capture spatial and temporal dependencies in a dataset through convolutional filters. A convolution kernel is local linear filter that is slid over the imput tensor along one or more dimensions to obtain a feature map as shown in Figure \ref{fig:CNN}. The general method for temporal CNN layer with a nonlinear activation function is

\hfil $C_t = f(W \cdot X_{t-l/2 \to t+l/2} + b) | \forall t \in [1, T], $ \par \

\begin{figure}[H]
  \centering
  \caption[Convolving on univariate input time series]{\emph{Convolving on univariate input time series \\
  Reprinted from \citeauthor{ismail_fawaz_forestier_weber_idoumghar_muller_2019} \citeyear{ismail_fawaz_forestier_weber_idoumghar_muller_2019}.}}\label{fig:CNN}
  \includegraphics[scale = 0.3]{figures/CNN.jpg}  
\end{figure}


where $C_t$ is the result of the convolution operation at time $t$ on time series $X$ of length $T$ with a filter $W$ of length $l$, a bias parameter $b$, and a final non-linear function $f$. It can be noticed that the same filter values $W$ and bias $b$ are used at every timestep, a very significant and useful property called weight sharing. When a series of convolutions are completed, the resulting feature maps would typically be fed though fully-connected layer as in the simple neural network architecture shown in Figure \ref{fig:CNN_ts}.

\begin{figure}[H]
  \centering
  \caption[Typical temporal convolutional neural network architecture.]{\emph{Typical temporal convolutional neural network architecture.  \\
  Reprinted from \citeauthor{ismail_fawaz_forestier_weber_idoumghar_muller_2019} \citeyear{ismail_fawaz_forestier_weber_idoumghar_muller_2019}.}}\label{fig:CNN_ts}
  \includegraphics[scale = 0.3]{figures/CNN_ts.jpg}  
\end{figure}



\section{Autoencoders}
\paragraph{}
An autoencoder is a neural network able to compress data similar to what it was trained on. Autoencoders do not require labeled data for training since they utilize unsupervised learning. We just feed the raw input into the model. Figure \ref{fig:ae} illustrates the intuition of how an autoencoder works. 

\paragraph{}
Besides compression, an autoencoder can be used for denoising by training the autoencoder to reproduce an original noiseless input given a noisy input. This allows the autoencoder to be flexible in the presence of white noise capturing only useful patterns in the data \cite{vincent10a}.

\begin{figure}[H]
  \centering
  \caption[An autocoder workflow.]{\emph{An autocoder workflow. \\
  Reprinted from \citeauthor{chollet_2016} \citeyear{chollet_2016}.}}\label{fig:ae}
  \includegraphics[scale = 0.5]{figures/ae.jpg}  
\end{figure}

\paragraph{}
An autoencoder has three main components \cite{badr_2019}: the encoder, the code or bottleneck, and the decoder, as shown in Figure \ref{fig:ae_architecture}.
\begin{itemize}
\item Encoder: Learns how to reduce input dimensionality, compressing the input data into an encoded representation.
\item Bottleneck: The layer that contains the compressed representation of the input data. This code space is also called the latent space.
\item Decoder: Learns how to reconstruct as closely as possible the input pattern from the encoded representation.
\end{itemize}

\begin{figure}[H]
  \centering
  \caption[The autoencoder architecture.]{\emph{The autoencoder architecture. \\
  Reprinted from \citeauthor{pedamkar_2019} \citeyear{pedamkar_2019}.}}\label{fig:ae_architecture}
  \includegraphics[scale = 0.5]{figures/ae_architecture.jpg}  
\end{figure}

\paragraph{}
The indirect benefits of this model is that it can be used for dimensionality reduction \cite{rajan_2021}. The bottleneck has the fewest units of any layer. An example of the kinds of compression an autoencoder can achieve is shown in Figure \ref{fig:ae_f_reduction}. This model reduces the three dimensional input to two dimensions.


\begin{figure}[H]
  \centering
  \caption[Visualization of dimensionality reduction using autoencoders.]{\emph{Visualization of dimensionality reduction using autoencoders. \\ 
  Reprinted from Johns Hopkins University \citeyear{johns_hopkins_university_2015}.}}\label{fig:ae_f_reduction}
  \includegraphics[scale = 0.2]{figures/ae_f_reduction.jpg}  
\end{figure}

\paragraph{}
Besides compression, an autoencoder can be used for denoising by training the autoencoder to reproduce an original noiseless input given a noisy input, as shown in Figure \ref{fig:ae_2}. This is because the optimizer encodes the inputs it was trained on as much as possible. \citeauthor{vincent_larochelle_bengio_manzagol_2008} \citeyear{vincent_larochelle_bengio_manzagol_2008} found that the robustness of the code at the bottleneck was improved by adding noise to the original input. This allows the autoencoder to be flexible in the presence of white noise capturing only useful patterns in the data \cite{vincent10a}.


\begin{figure}[H]
  \centering
  \caption[An autoencoder trained on "clean" images can correct noisy input.]{\emph{An autoencoder trained on ``clean'' images can correct noisy input. \\
  Reprinted from \citeauthor{rosebrock_2020} \citeyear{rosebrock_2020}.}}\label{fig:ae_2}
  \includegraphics[scale = 0.4]{figures/ae_2.jpg}  
\end{figure}

\paragraph{}
Beside the simple feedforward layers described previously, autoencoder can be combined with long short term memory networks (LSTMs) or convolutional neural networks (CNNs) depending on the type of input. In my thesis, the input, vibration from human activities, is a sequential time series. Therefore, an combination of autoencoder with LSTM networks may be suitable for my purpose.

\subsection{Autoencoders for Anomaly Detection}

\paragraph{}
Autoencoders are extremely useful as methods of typicality. Consider a person who does the same things every day. Suppose that one day, an unusual event occurs. An autoencoder trained on the usual daily activities will map the new situation to something similar in the training, as shown in Figure \ref{fig:ae_detection}. The reconstruction error in abnormal causes should be high. A model trained on one type of data (the normal activities) will fail when facing abnormal data it has never seen before. The simple autoencoder-based anomaly detection algorithm is shown in Algorithm \ref{al:anomaly_detection_algorithm}.

\begin{figure}[H]
  \centering
  \caption[An autoencoder capable of detecting anomalous events in time series.]{\emph{An autoencoder capable of detecting anomalous events in time series. \\ Reprinted from \citeauthor{pavithrasv_2020} \citeyear{pavithrasv_2020}.}}\label{fig:ae_detection}
  \includegraphics[scale = 0.2]{figures/ae_detection.jpg}  
\end{figure}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\renewcommand{\algorithmicforall}{\textbf{for each}}

\begin{algorithm}[H]
\caption{Autoencoder-based anomaly detection }
\label{al:anomaly_detection_algorithm}
\begin{algorithmic}
  \REQUIRE Normal dataset: $X^{(i)} (i = 1,..., m)$, abnormal dataset: $x^{(j)}$ $(j = 1, ..., n)$, threshold: $\alpha$
  \ENSURE Reconstruct data: $\hat{X}$
  \STATE Reconstruction error: $\| X - \hat{X} \|$ 
  \STATE Train an autoencoder using the normal dataset X $\rightarrow$ $L^* = \underset{L} {\operatorname{argmin}} \sum_{i=1}^m \| {X}^{(i)} - \hat X^{(i)} \| ^ 2$
  \STATE Testing an autoencoder:
  \STATE \textbf{for} j = 1 to $n$ \textbf{do}:
  \STATE \indent \textbf{if} reconstruction error $<$ $\alpha$:
  \STATE \indent \indent $x^{(j)}$ is a nomal.
  \STATE \indent \textbf{else}:
  \STATE \indent \indent $x^{(j)}$ is an anomaly.
\end{algorithmic}
\end{algorithm}

\subsection{Variational Autoencoder}
\paragraph{}
The variational autoencoder (VAE) is a generative model like the ordinary autoencoder, it encodes and decodes data in the training set, but it also attempts to model the probability density over the input space of the examples emitted by the data source, by transforming, e.g., Gaussian distributied latent vectors to elements of the input space. For example, if model is trained with traffic images, the decoder, when passed a sample from the code space, would have a high probability of emitting vehicle images object related to traffic. Other data would have a low probability of being emitted. By sampling in the latent space reconstructing, the variational autoencoder can also generate new examples that look similar to those from the original dataset \cite{roger_2021}. In the other words, a variational autoencoder is an encoder which is trained to be regularized at the bottomneck in order to guarantee that latent space is a good source for the generative process \cite{rocca_2020}. The architecture of a variational autoencoder is illustrated in Figure \ref{fig:VAE}. The latent space of a variational autoencoder is easy to sample from.

\begin{figure}[H]
  \centering
  \caption[Architecture of a variational autoencoder.]{\emph{Architecture of a variational autoencoder. \\
  Reprinted from \citeauthor{weng_2018} \citeyear{weng_2018}.}}\label{fig:VAE}
  \includegraphics[scale = 0.15]{figures/VAE.jpg}  
\end{figure}

\paragraph{}
The principals mentioned above do not mean that a variational autoencoder always has better performance than general autoencoders in anomaly detection tasks \cite{agmon_2021}, since the objective of the variational autoencoder is as a generative model for new data.


\section{Recurrent Neural Network (RNN)}
\paragraph{}
The main idea of Recurrent Neural Network (RNN) is to apply sequential data such as video (sequence of images) or text (sequence of word). For example, when people are reading a book, it is a sequence of words because we read a book from left to right. That we can know what the sentence we are reading is about. We take the story from what we have read in the past, it is a hidden state, and mix it with the words we just read (input data or the words we are reading at that time). RNN uses the same principle, which is to modify the format of the old neural network so that the previous state or knowledge can be added to the new input data to understand something in a sequential time series \cite{donges_2019}. A key attribute of recurrent neural networks is their ability to persist information, or cell state, for use later in the network. There are 2 significant components of RNN as hidden state and input data.


\begin{figure}[H]
  \centering
   \caption[The Recurrent Neural Network architecture.]{\emph{The Recurrent Neural Network architecture. \\ 
   Reprinted from \citeauthor{olah_2015} \citeyear{olah_2015}}}\label{fig:RNN}
  \includegraphics[scale = 0.2]{figures/RNN.jpg}  
\end{figure}

where $X_t$ is input data at time $t$, $A$ is Hidden layer, and $h_t$ is an output from RNN at time $t$ shown in Figure \ref{fig:RNN}. The main benefit of this loop is to bring back the previous hidden state, or simply say that RNN is a Neural Network with more memory to store the previously calculated hidden state. 

\paragraph{}
The main problem of RNN is its gradient. For those who have experienced in neural networks would clearly know that to update weights we use a backpropagation \cite{arnx_2019}, which calculates the gradient of the loss function ($E$) to update the weights which is shown in Figure \ref{fig:bpp}, but RNN is a bit more complicated, because getting the output $h_t$ is not only from the interval $t=t$, but also from $t = t-1, t-2, ...,t=1$. Therefore, backpropagation has to be included in all calculations from $t=1$ to $t=t$. Then If the gradient value is less than 1, long continuous multiplications like this will cause the gradient to decrease as the length of its sequence. In explicit, the RNN still has a problem with the data that the sequence is too long.

\begin{figure}[H]
  \centering
  \caption[The concept of optimization in a feed-forward neural network.]{\emph{The concept of optimization in a feed-forward neural network. \\Reprinted from \citeauthor{donges_2019} \citeyear{donges_2019}}}\label{fig:bpp}
  \includegraphics[scale = 0.4]{figures/bpp.jpg}  
\end{figure}

\begin{figure}[H]
  \centering
  \caption[The repeating module in a standard RNN contains a single layer.]{\emph{The repeating module in a standard RNN contains a single layer. \\Reprinted from \citeauthor{olah_2015} \citeyear{olah_2015}}}\label{fig:RNN_2}
  \includegraphics[scale = 0.2]{figures/RNN_2.jpg}  
\end{figure}

\section{Long Short-Term Memory (LSTM)}
\paragraph{}
Long short-term memory networks are an extension for recurrent neural networks, which basically extends the memory. Therefore it is well suited to learn from important experiences that have very long time lags in between \cite{donges_2019,olah_2015}. In addition, memory can also have a descriptor when should write, forget (delete) or read as shown in  Figure \ref{fig:LSTM}.

\begin{figure}[H]
  \centering
  \caption[The procedure inside the LSTM.]{\emph{The procedure in the LSTM. \\ Reprinted from  \citeauthor{sirinart_tangruamsub_2017} \citeyear{sirinart_tangruamsub_2017}}}\label{fig:LSTM}
  \includegraphics[scale = 0.3]{figures/LSTM.jpg}  
\end{figure}

\paragraph{}
Before getting into the working of LSTM, there are some variables which should be known as following \cite{sirinart_tangruamsub_2017}:
\begin{itemize}
\item Cell state: Store the memory state of memory cell the LSTM
\item Gate: Control the flow of data, (i.e. analog values) that control when should write to allow data flow in, read to allow data flow out or forget.
\end{itemize}
\paragraph{}
To be more clear, we will explain each functional gate one by one as following :

\paragraph{}
Forget : Forget is like clearing the old cell state, and preparing to clear memory for the new input. The person who decides whether to delete or not delete is a rule of forget gate. If the forget gate returns 0, then delete the previous cell state. If the forget gate returns 1, the model is going to store this cell state further. To create this forget gate, the model is going to look at the incoming input data with the previous hidden state (according to the RNN formula) for making decisions. The sigmoid function is used as shown in the equation below.

\hfil $ f_t = \sigma(W_{x^f}x_t + W_{h^f}h_{t-1} + b_f) $ \par 

  \paragraph{}
Write : When the new input is fed to the model, it will raise up 2 possible questions. Firstly, should the model update its cell state? This action is controlled by an input gate which still uses the sigmoid function. This computation which is shown below uses the incoming input data value and the previous hidden state.

\hfil $ i_t = \sigma(W_{x^i}x_t + W_{h^i}h_{t-1} + b_i) $ \par
Secondly, If the model really updates, what value should it update? It is called “Input modulation date” to handle. The equation which is shown below is similar to the input gate, but uses a tanh function instead.

\hfil $ g_t = \tanh(W_{x^c}x_t + W_{h^c}h_{t-1} + b_c) $ \par 

\paragraph{}
Update cell state : Currently, we get information from the forget gate, input gate and input modulation gate which are enough to update cell state. The equation of update cell state will be shown below.

\hfil $c_t = f_t \cdot c_{t-1} + i_t \cdot g_t $ \par 
First part, If the forget gate wants to delete the old cell state ($f_t$ is 0), the model will not let $c_{t -1}$ to update cell state anymore. But if $f_t$ is 1, the model can still keep $c_{t -1}$ to be considered. Second part, this section will update the cell state from the new data. Now that model has the values to be updated and waited from the input modulation gate or $g_t$. If $i_t$ is 1, then use $g_t$ to update. Otherwise, $g_t$ is overlooked.

\paragraph{}
Read : From the original RNN, what the model needs to produce is the hidden state at time $t$ or $h_t$. At the time of $t+1$, this LSTM takes this $h_t$ to be calculated. Therefore, the word “read” means to allow outsiders to read the $h_t$ or not. Or it will not pass the $h_t$ value. We have an output gate to help the model decide as shown in the equation below. 

\hfil $ o_t = \sigma(W_{x^o}x_t + W_{h^o}h_{t-1} + b_o) $ \par 
And the output will be $h_t$ for the next sequence

\hfil $ h_t = o_t \cdot \tanh(c_t) $ \par 
If the output gate provides $o_t$ with 0 value, then $h_t$ is 0 (meaning nothing is sent). Meanwhile, if $o_t$ is 1, the model computes $h_t$ and sends it outside or simply says that it allows others to see the $h_t$ value.

\begin{figure}[H]
  \centering
  \caption[The repeating module in a LSTM contains four interacting layer.]{\emph{The repeating module in a LSTM contains four interacting layer. \\Reprinted from \citeauthor{sirinart_tangruamsub_2017} \citeyear{sirinart_tangruamsub_2017}}}\label{fig:LSTM_2}
  \includegraphics[scale = 0.2]{figures/LSTM_2.jpg}  

\end{figure}

\section{Transformer}
\paragraph{}
The Transformer was proposed in the paper Attention is All You Need by Google \cite{vaswani_shazeer_parmar_uszkoreit_jones_n_gomez_kaiser_polosukhin_2017}. This paper proposes a new architecture that replaces RNNs with attention called Transformer as shown in Figure \ref{fig:attention}. Transformer architecture has continued to beat benchmarks in many domains. Explicitly, it has revolutionized the Natural Language Processing (NLP) field particularly on the machine learning task. This model contains 2 significant parts as an encoder and decoder which can work as similar as an autoencoder. Thus, it can be used for anomaly detection purposes as well.

\begin{figure}[H]
  \centering
  \caption[The Transformer architecture.]{\emph{The Transformer architecture. \\Reprinted from \citeauthor{vaswani_shazeer_parmar_uszkoreit_jones_n_gomez_kaiser_polosukhin_2017} \citeyear{vaswani_shazeer_parmar_uszkoreit_jones_n_gomez_kaiser_polosukhin_2017}}}\label{fig:attention}
  \includegraphics[scale = 0.4
  ]{figures/attention.jpg}  
\end{figure}

\paragraph{}
Let’s compare RNNs and attention, RNNs include every information that they had known about a sequential data into the final hidden state of the network. Thus, the decision layer can access only the memory layer which is related to that time step. It means that at every time step, it focuses on different positions on the other RNN. On the other hand, an attention mechanism regards the input from several time steps and sets different weights to each input to know which input should be focused in order to make one prediction. In Figure \ref{fig:rnnvsattention}, this image is going to provide simple intuition of both methods.

\begin{figure}[H]
  \centering
  \caption[Comparison RNNs and Attention.]{\emph{Comparison RNNs and Attention.}}\label{fig:rnnvsattention}
  \includegraphics[scale = 0.4
  ]{figures/rnnvsattention.jpg}  
\end{figure}
\subsection{Attention}
\paragraph{}
In psychology, attention is  a concentration of mind on a single object or thought, especially one preferentially selected from a complex, with a view to limiting or clarifying receptivity by narrowing the range of stimuli. Similarly, attention was specifically designed to focus on only the most important subsets of long sequences which are related to completeness of a given task \cite{alammar_2018,alammar_2019,klingenbrunn_2021}. It actually consists 3 main steps as following:
\begin{enumerate}
\item Create the Query, Key, and Value vectors for each path and each input token by multiplying by weight matrices as $W^Q$, $W^K$ and $W^V$ as shown in Figure \ref{fig:attention_2}.
\item For each input token, use its query vector to get a score against all the other key vectors by multiplying the current Query vector with all the Key vectors as shown in Figure \ref{fig:attention_3}.
\item Sum up the Value vectors after multiplying them by their associated scores. The more transparent means lower value as shown in Figure \ref{fig:attention_4}.
\end{enumerate}

\paragraph{}
Therefore, If the model does the same operation for each input token, it is going to finish with a vector which represents the appropriate context of each token as shown in Figure \ref{fig:attention_5}. And these vectors are going to the next sub layer in the transformer block which must be fed into the forward neural network.
\begin{figure}[H]
  \centering
  \caption[Create the query, key and value vector.]{\emph{Create the query, key and value vector. \\ 
  Reprinted from \citeauthor{alammar_2018} \citeyear{alammar_2018}}}\label{fig:attention_2}
  \includegraphics[scale = 0.3]{figures/attention_2.jpg}  
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Get score of how they match.]{\emph{Get score of how they match. \\ 
  Reprinted from \citeauthor{alammar_2018} \citeyear{alammar_2018}}}\label{fig:attention_3}
  \includegraphics[scale = 0.3]{figures/attention_3.jpg}  
\end{figure}

\begin{figure}[H]
  \centering
  \caption[Sum up the value vectors.]{\emph{Sum up the value vectors. \\
  Reprinted from \citeauthor{alammar_2018} \citeyear{alammar_2018}}}\label{fig:attention_4}
  \includegraphics[scale = 0.3]{figures/attention_4.jpg}  
\end{figure}
\begin{figure}[H]
  \centering
  \caption[The outcome after finishing the Attention process.]{\emph{The outcome after finishing the Attention process. \\Reprinted from \citeauthor{alammar_2018} \citeyear{alammar_2018}}}\label{fig:attention_5}
  \includegraphics[scale = 0.3]{figures/attention_5.jpg}  
\end{figure}

\paragraph{}
In addition, it can be noticed that in the decoder, it contains Masked Self-Attention. It is very important that the difference between Self-Attention and Masked Self-Attention is quite clear when you look at Figure \ref{fig:attention_6}. A Self-Attention allows each position to attend to all positions from input but Masked Self-Attention only considers the previous position and including that position in order to preserve the auto-regressive property.

\begin{figure}[H]
  \centering
  \caption[The difference of Self-Attention and Masked Self-Attention.]{\emph{The difference of Self-Attention and Masked Self-Attention. \\ Reprinted from \citeauthor{alammar_2019} \citeyear{alammar_2019}}}\label{fig:attention_6}
  \includegraphics[scale = 0.3]{figures/attention_6.jpg}  
\end{figure}
\subsection{Positional Encoding}
\paragraph{}
For sequence to sequence model, orders and position are important. Since the model does not have recurrence and convolution, we have to add some information to make use of the order in the sequence. You can see in Figure \ref{fig:attention_7}.

\begin{figure}[H]
  \centering
  \caption[The dimension of each positional encoding and embeddings.]{\emph{The dimension of each positional encoding and embeddings. \\ Reprinted from \citeauthor{alammar_2018} \citeyear{alammar_2018}}}\label{fig:attention_7}
  \includegraphics[scale = 0.4]{figures/attention_7.jpg}  
\end{figure}


\FloatBarrier